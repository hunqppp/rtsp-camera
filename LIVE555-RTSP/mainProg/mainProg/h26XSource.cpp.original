#include "h26XParser.h"
#include "h26XSource.hh"
#include <GroupsockHelper.hh>
#include "avSource.h"

const char *VideoReceiver::UNIX_SOCKET_VIDEO_PATH = (const char*)"/tmp/video.sock";

int VideoReceiver::fd = -1;
socklen_t VideoReceiver::addrClientLen;
struct sockaddr_un VideoReceiver::addrSocket;
struct sockaddr_un VideoReceiver::addrClient;


static std::vector<uint8_t> samples;

VVTK_RET_CALLBACK H26XSource::videoSampleCapture(const vvtk_video_frame_t *videoFrame, const void *arg) {
    (void)arg;
	if (videoFrame->size) {
		
	}

    return VVTK_RET_CALLBACK_CONTINUE;
}

void VideoReceiver::doOpen() {
    if (VideoReceiver::fd != -1) {
        return;
    }
	VideoReceiver::addrClientLen = sizeof(VideoReceiver::addrClient);
	
	if ((VideoReceiver::fd = socket(AF_UNIX, SOCK_DGRAM, 0)) == -1){
		return;
	}

	memset(&VideoReceiver::addrSocket, 0, sizeof(VideoReceiver::addrSocket));
	VideoReceiver::addrSocket.sun_family = AF_UNIX;
	strncpy(VideoReceiver::addrSocket.sun_path, UNIX_SOCKET_VIDEO_PATH, sizeof(VideoReceiver::addrSocket.sun_path) - 1);

	unlink(UNIX_SOCKET_VIDEO_PATH);
	if (bind(VideoReceiver::fd, (struct sockaddr *)&VideoReceiver::addrSocket, sizeof(VideoReceiver::addrSocket)) == -1) {
		close(VideoReceiver::fd);
		return;
	}
}

void VideoReceiver::doClose() {
    if (VideoReceiver::fd != -1) {
		close(VideoReceiver::fd);
	}
}

size_t VideoReceiver::doReceive(unsigned char **p, uint32_t MaxSize) {
    size_t frameSize = 0;
    if (VideoReceiver::fd != -1) {
        frameSize = recvfrom(VideoReceiver::fd, *p, MaxSize, 0, (struct sockaddr*)&VideoReceiver::addrClient, &VideoReceiver::addrClientLen);
	}

    return frameSize;
}

H26XSource *
H26XSource::createNew(UsageEnvironment &env)
{
    return new H26XSource(env);
}

EventTriggerId H26XSource::eventTriggerId = 0;

unsigned H26XSource::referenceCount = 0;

H26XSource::H26XSource(UsageEnvironment &env) : FramedSource(env)
{
    static bool boolean = true;
    if (boolean) {
        boolean = false;
        vvtk_set_video_callback(1, this->videoSampleCapture, NULL);
    }

    if (referenceCount == 0)
    {
        // Any global initialization of the device would be done here:
        //%%% TO BE WRITTEN %%%
        VideoReceiver::doOpen();
    }
    ++referenceCount;

    // Any instance-specific initialization of the device would be done here:
    //%%% TO BE WRITTEN %%%

    // We arrange here for our "deliverFrame" member function to be called
    // whenever the next frame of data becomes available from the device.
    //
    // If the device can be accessed as a readable socket, then one easy way to do this is using a call to
    //     envir().taskScheduler().turnOnBackgroundReadHandling( ... )
    // (See examples of this call in the "liveMedia" directory.)
    //
    // If, however, the device *cannot* be accessed as a readable socket, then instead we can implement it using 'event triggers':
    // Create an 'event trigger' for this device (if it hasn't already been done):
    if (eventTriggerId == 0)
    {
        eventTriggerId = envir().taskScheduler().createEventTrigger(deliverFrame0);
    }
}

H26XSource::~H26XSource()
{
    // Any instance-specific 'destruction' (i.e., resetting) of the device would be done here:
    //%%% TO BE WRITTEN %%%

    --referenceCount;
    if (referenceCount == 0)
    {
        // Any global 'destruction' (i.e., resetting) of the device would be done here:
        //%%% TO BE WRITTEN %%%
        // UNIX_CloseSocket();

        // Reclaim our 'event trigger'
        envir().taskScheduler().deleteEventTrigger(eventTriggerId);
        eventTriggerId = 0;
    }
}

void H26XSource::doGetNextFrame()
{
    // This function is called (by our 'downstream' object) when it asks for new data.

    // Note: If, for some reason, the source device stops being readable (e.g., it gets closed), then you do the following:
    if (0 /* the source stops being readable */ /*%%% TO BE WRITTEN %%%*/)
    {
        handleClosure();
        return;
    }

    deliverFrame();

    // No new data is immediately available to be delivered.  We don't do anything more here.
    // Instead, our event trigger must be called (e.g., from a separate thread) when new data becomes available.
}

void H26XSource::doStopGettingFrames() {

}

void H26XSource::deliverFrame0(void *clientData)
{
    ((H26XSource *)clientData)->deliverFrame();
}

void H26XSource::deliverFrame()
{
    // This function is called when new frame data is available from the device.
    // We deliver this data by copying it to the 'downstream' object, using the following parameters (class members):
    // 'in' parameters (these should *not* be modified by this function):
    //     fTo: The frame data is copied to this address.
    //         (Note that the variable "fTo" is *not* modified.  Instead,
    //          the frame data is copied to the address pointed to by "fTo".)
    //     fMaxSize: This is the maximum number of bytes that can be copied
    //         (If the actual frame is larger than this, then it should
    //          be truncated, and "fNumTruncatedBytes" set accordingly.)
    // 'out' parameters (these are modified by this function):
    //     fFrameSize: Should be set to the delivered frame size (<= fMaxSize).
    //     fNumTruncatedBytes: Should be set iff the delivered frame would have been
    //         bigger than "fMaxSize", in which case it's set to the number of bytes
    //         that have been omitted.
    //     fPresentationTime: Should be set to the frame's presentation time
    //         (seconds, microseconds).  This time must be aligned with 'wall-clock time' - i.e., the time that you would get
    //         by calling "gettimeofday()".
    //     fDurationInMicroseconds: Should be set to the frame's duration, if known.
    //         If, however, the device is a 'live source' (e.g., encoded from a camera or microphone), then we probably don't need
    //         to set this variable, because - in this case - data will never arrive 'early'.
    // Note the code below.

    if (!isCurrentlyAwaitingData()) {
        return; // we're not ready for the data yet
    }

#if 0
    u_int8_t newFrameDataStart[50 * 1024];
	unsigned newFrameSize = sizeof(newFrameDataStart);

	do {
		newFrameSize = recvfrom(fd, newFrameDataStart, newFrameSize, 0, (struct sockaddr*)&addrClient, &addrClientLen);
		printf("Sample H264 read from socket size: %d\n", newFrameSize);
	} while (newFrameSize < 0);

    uint8_t *newFrameDataStart = buffer;
    uint32_t newFrameSize = bufferSize;
    
    // Deliver the data here:
    if (newFrameSize > fMaxSize)
    {
        fFrameSize = fMaxSize;
        fNumTruncatedBytes = newFrameSize - fMaxSize;
    }
    else
    {
        fFrameSize = newFrameSize;
    }
    
    gettimeofday(&fPresentationTime, NULL); // If you have a more accurate time - e.g., from an encoder - then use that instead.
    // If the device is *not* a 'live source' (e.g., it comes instead from a file or buffer), then set "fDurationInMicroseconds" here.
    memmove(fTo, newFrameDataStart, fFrameSize);
#else
    size_t newFrameSize = 0;
    newFrameSize = VideoReceiver::doReceive(&fTo, fMaxSize);

    // while (samples.empty());
    // size_t newFrameSize = samples.size();
    // memmove(fTo, samples.data(), samples.size());
    // samples.clear();

#ifdef SAVE_IMAGE
    if (newFrameSize > (15 * 1024)) {
        rtc::binary nalu;
        nalu.insert(nalu.end(),
                    reinterpret_cast<uint8_t*>(fTo),
                    reinterpret_cast<uint8_t*>(fTo) + newFrameSize);
        H26X_DumpNalusToURL("/tmp/nfs/image.h264", nalu);
        exit(EXIT_SUCCESS);
    }
#endif

#if 1
    printf("Sample H264 read from socket size: %d\n", newFrameSize);
#endif

    // Deliver the data here:
    if (newFrameSize > fMaxSize)
    {
        fFrameSize = fMaxSize;
        fNumTruncatedBytes = newFrameSize - fMaxSize;
    }
    else
    {
        fFrameSize = newFrameSize;
    }

    gettimeofday(&fPresentationTime, NULL);
#endif

    // After delivering the data, inform the reader that it is now available:
    FramedSource::afterGetting(this);
}

// The following code would be called to signal that a new frame of data has become available.
// This (unlike other "LIVE555 Streaming Media" library code) may be called from a separate thread.
// (Note, however, that "triggerEvent()" cannot be called with the same 'event trigger id' from different threads.
// Also, if you want to have multiple device threads, each one using a different 'event trigger id', then you will need
// to make "eventTriggerId" a non-static member variable of "H26XSource".)
void signalNewFrameData()
{
    TaskScheduler *ourScheduler = NULL; //%%% TO BE WRITTEN %%%
    H26XSource *ourDevice = NULL;       //%%% TO BE WRITTEN %%%

    if (ourScheduler != NULL)
    { // sanity check
        ourScheduler->triggerEvent(H26XSource::eventTriggerId, ourDevice);
    }
}
